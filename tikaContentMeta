import os
from tika import parser
from multiprocessing import Pool

#
# Build Dependencies
#
# datalake becomes S3/bucketIn
# datalake becomes S3/bucketOut
dataLakein = 'e:/S3/bucketIn/'
metatext ='e:/S3/bucketOut/metadata.txt'
contentext ='e:/S3/bucketOut/contentdata.txt'
listOfFileNames = [f for f in os.listdir(dataLakein)]
listOfFilePaths = [dataLakein + file for file in listOfFileNames]

#
#Create Function
# 
def tika_metaparser(input):
   #Extract metadata text from document
   content = parser.from_file(input)   
   metatext = content['metadata']

   #Convert to string
   metatext = str(metatext)   
  
   # Ensure text is utf-8 formatted
   safe_text = metatext.encode('utf-8', errors='ignore') 
   return safe_text  

def tika_contentparser(input):
   content = parser.from_file(input)   
   contenttext = content['content']

   #Convert to string
   contenttext = str(contenttext)   
  
   # Ensure text is utf-8 formatted
   safe_text = contenttext.encode('utf-8', errors='ignore') 
   return safe_text  
  
#Execute Functon with Multiprocessing
#   
if __name__ == '__main__':
   with Pool(5) as p:
        print(p.map(tika_metaparser, listOfFilePaths))        
        #get the data from the datalake and write it to my outfile.
        #
        # Place results in S3 bucket out. 
        #
        data=(p.map(tika_metaparser, listOfFilePaths))              
        with open(metatext, 'w') as data_out:
           print(data, file=data_out)
           
        datatxt=(p.map(tika_contentparser, listOfFilePaths))              
        with open(contentext, 'w') as datatxt_out:
           print(datatxt, file=datatxt_out)
      
      
